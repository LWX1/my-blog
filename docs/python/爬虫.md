---
title: 爬虫
date: 2023-03-15
isShowComments: false
tags:
    - python
categories:
    - python
---

## 前期准备

### 封装IP池

```
<!-- 判断IP是否有用 -->
def isUseIP(proxoes_dict):
    try:
        result = requests.get(url="https://www.baidu.com/", proxies=proxoes_dict, timeout=1)
        if result.status_code == 200:
            # print("可用")
            return True
        else:
            return False
    except Exception as e:
        print("无效：", proxoes_dict)
        return False


class IPProxy:
    def __int__(self):
        self.proxyList = []
        self.currentIP = None
        self.readIP()

    # 读入IP
    def readIP(self):
        <!-- 前期准备好爬取的IP数据 -->
        with open("data.json", 'r', encoding="utf-8") as f:
            text = f.read()
        dataList = json.loads(text)
        for item in dataList:
            self.proxyList.push(item)

    # 获取IP
    def getIP(self):
        while True:
            if len(self.currentIP) == 0:
                return None
            self.currentIP = random.choice(self.proxyList)
            bool = isUseIP({
                [self.currentIP.type]: self.currentIP.usr
            })
            if not bool:
                self.proxyList.remove(self.currentIP)
                self.getIP()
            else:
                return self.currentIP

    # 重新保存未过期的IP
    def saveIP(self):
        with open("data.json", "w", encoding="utf-8") as f:
            f.write(json.dumps(self.proxyList, indent=4, ensure_ascii=False))

<!-- 使用代理IP -->
def getProxyIP():
    proxy = new IPProxy()
    IP = proxy.getIP()

```
### 常规爬虫

- 主要通过访问网页地址，再解析网页，获取相对应的数据

#### 请求头

```
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
}
```

#### 请求数据
```
<!-- 引入包 -->
import requests
from bs4 import BeautifulSoup

<!-- 获取网页数据 -->
def getText(url):
    <!-- 使用代理IP,防止被封IP -->
    IP = getProxyIP();
    res = requests.get(url, headers=headers, proxies={"http": IP})
    content = res.text.encode('utf-8')
    soup = BeautifulSoup(content, 'lxml')
    return soup
```

```
<!-- 设置格式数据 -->
def setData(dataList, index, result):
    for item in dataList:
        index = index + 1
        result.append({
            "title": item["word"],
            "hot": item["hotScore"],
            "index": index,
            "url": item["url"],
            "desc": item["desc"],
            "pic": item["img"],
        })
    return result, index

<!-- 获取百度热点数据 -->
def baiduHot(request):
    soup = getText("https://top.baidu.com/board?tab=realtime");
    result = []
    <!-- 解析你想要的数据 -->
    content = soup.find(name="div", attrs={"id": "sanRoot"})

    content = str(content)

    reg = re.compile(r"<!--s-data:(.*?)-->")

    text = reg.findall(content)

    jsonData = json.loads(text[0])
    cardsData = jsonData["data"]["cards"][0]
    dataContent = cardsData["content"]
    index = 0;
    result, index = setData(dataContent, index, result)
    return result
```

### 自动化爬虫

- 虽然也可以通过解析页面获取相应的数据，但是自动化爬虫主要用于自动化操作业务；例如现在比较热门的自动化抢票；
  
#### 自动化购票
- 给出关键代码，具体代码为机密
```
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

<!-- 获取浏览器驱动 -->
def getChrome():
    driver = webdriver.Chrome(options=self.chrome_options)  # 默认谷歌浏览器, 指定下驱动的位置
    return driver
```

```
driver = getChrome()

<!-- 自动化开启，打开某个链接 -->
driver.get(url)
<!-- 查找到相应的元素并点击 -->
buy = driver.find_element(By.XPATH, '//*[@id="bottom"]/div[2]/div[1]/div[1]/div/div')
buy.click()
```

```
<!-- 遇到一些延迟的节点元素 -->
<!-- 等待元素1 0.01秒到10秒，直到出现./div[2]/div[2]/div 的元素 -->
WebDriverWait(元素1,10,0.01).until(EC.presence_of_all_elements_located((By.XPATH,'./div[2]/div[2]/div')))
```
### 逆向爬虫

- 通过直接获取请求来获取数据，更加高效；但是也会存在一些问题，有些请求参数会加密，获取到相关的数据难度更大；

#### 解析参数

- 目前最多的网站就是使用webpack，我们需要找到入口，再进行定位抠代码就可以获取到相关的参数加密了；
  
#### 有道翻译

- 进行了md5加密，里面有个sign参数，我们抠相应的代码，就可以进行参数解密，使用execjs 库解析获取到sign的值

```
import execjs
# 读取外部 JavaScript 文件
with open('getSign.js', 'r', encoding='utf-8') as f:
    js_code = f.read()
context = execjs.compile(str(js_code))
def getSign(date):
    data = "client=fanyideskweb&mysticTime="+str(date)+"&product=webfanyi&key=fsdsogkndfokasodnaso"
    return context.call('md5_encrypt', data)

```


### 言后

- 逆向虽然有点难度，但是也是我们需要的，如实现第三方系统的单点登录，我们就可以去抠登录接口，获取到token，通过nginx代理就可以实现另类的单点登录。

- 我们还可以通过逆向去抓取很多接口，集成到自己的系统里，如有道翻译，IP查询，抖音视频等等